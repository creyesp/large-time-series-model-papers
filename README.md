# Large time series model papers

|     | Name | Institution | Date | Dataset | Size |
|-----|------|-------------| -- | -- | -- |
| <ul><li> [x] </li></ul> | [TimeFM: A decoder-only foundation model for time-series forecasting](https://arxiv.org/abs/2310.10688) | `Google` | Oct 2023 | 2B | 200M |
| <ul><li> [x] </li></ul> | [TimeGPT-1](https://arxiv.org/abs/2310.03589) | `Nixtla` | Oct 2023 | 100B |  |
| <ul><li> [x] </li></ul> | [MOMENT: A Family of Open Time-series Foundation Models](https://arxiv.org/abs/2402.03885) | `CMU` | Feb 2024 | | |
| <ul><li> [x] </li></ul> | [MOIRAI: Unified Training of Universal Time Series Forecasting Transformers](https://arxiv.org/abs//2402.02592) | `SalesforceAI` | Feb 2024 | 27B | 14m, 91m, 311m |
| <ul><li> [ ] </li></ul> | [ForecastPFN]() |  |  |  | |
| <ul><li> [ ] </li></ul> | [Lag-Llama]() |  |  | 1B | |
| <ul><li> [ ] </li></ul> | [Tiny Time Mixers (TTMs)]() |  |  |  | |
| <ul><li> [ ] </li></ul> | [LLMTime]() |  |  |  | |




