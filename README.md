# Large time series model papers

|     | Name | Institution | Date | Dataset | Size | Code |
|-----|------|-------------| -- | -- | -- | -- |
| <ul><li> [x] </li></ul> | [TimeFM: A decoder-only foundation model for time-series forecasting](https://arxiv.org/abs/2310.10688) | `Google` | Oct 2023 | 2B | 200M | [checkpoint](https://huggingface.co/google/timesfm-1.0-200m) |
| <ul><li> [x] </li></ul> | [TimeGPT-1](https://arxiv.org/abs/2310.03589) | `Nixtla` | Oct 2023 | 100B |  | |
| <ul><li> [x] </li></ul> | [MOMENT: A Family of Open Time-series Foundation Models](https://arxiv.org/abs/2402.03885) | `CMU` | Feb 2024 | | | [code](https://moment-timeseries-foundation-model.github.io/) |
| <ul><li> [x] </li></ul> | [MOIRAI: Unified Training of Universal Time Series Forecasting Transformers](https://arxiv.org/abs//2402.02592) | `SalesforceAI` | Feb 2024 | 27B | 14m, 91m, 311m | [checkpoint](https://huggingface.co/Salesforce/moirai-1.0-R-large) [blog](https://blog.salesforceairesearch.com/moirai/) |
| <ul><li> [ ] </li></ul> | [ForecastPFN]() |  |  |  | | |
| <ul><li> [ ] </li></ul> | [Lag-Llama]() |  |  | 1B | | |
| <ul><li> [ ] </li></ul> | [Tiny Time Mixers (TTMs)]() |  |  |  | | |
| <ul><li> [ ] </li></ul> | [LLMTime]() |  |  |  | | |
| <ul><li> [ ] </li></ul> | [Chronos](https://arxiv.org/abs/2403.07815) | `Amazon` | March 2024 |  | 8M, 20M, 46M, 200M, 710M | [code](https://github.com/amazon-science/chronos-forecasting)|
| <ul><li> [ ] </li></ul> | [VISIONTS](https://arxiv.org/pdf/2408.17253) | `Saleforce` | Agust 2024 |  | | |
